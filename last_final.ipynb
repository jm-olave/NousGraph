{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qj6zZ9K9R_wk"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    default_data_collator,\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    f1_score,\n    accuracy_score,\n    hamming_loss,\n    precision_score,\n    recall_score,\n)\nfrom torch.utils.data import Dataset\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\nimport random"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X0DbUjlWSJCR"
   },
   "outputs": [],
   "source": "def check_gpu():\n    \"\"\"Check GPU availability\"\"\"\n    if torch.cuda.is_available():\n        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n        print(\n            f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\"\n        )\n    else:\n        print(\"WARNING: No GPU available\")\n\n\ndef load_model_and_tokenizer():\n    \"\"\"Load PubMedBERT model and tokenizer\"\"\"\n    model_name = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_name, num_labels=4, problem_type=\"multi_label_classification\"\n    )\n\n    print(f\"Model loaded: {model.num_parameters():,} parameters\")\n    return tokenizer, model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sN6qAaOsNtCv"
   },
   "outputs": [],
   "source": "def analyze_text_lengths(df, tokenizer):\n    \"\"\"Analyze text lengths for optimal max_length\"\"\"\n    lengths = df[\"text\"].apply(lambda x: len(tokenizer.encode(str(x))))\n\n    print(f\"\\nText length analysis:\")\n    print(f\"  Mean: {lengths.mean():.0f} tokens\")\n    print(f\"  95th percentile: {lengths.quantile(0.95):.0f} tokens\")\n    print(f\"  Max: {lengths.max():.0f} tokens\")\n\n    optimal_length = min(512, int(lengths.quantile(0.95)))\n    print(f\"  Recommended max_length: {optimal_length}\")\n\n    return optimal_length"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Ab9xRmnOVZa"
   },
   "outputs": [],
   "source": "def compute_multilabel_metrics(eval_pred):\n    \"\"\"Compute comprehensive multi-label metrics\"\"\"\n    predictions, labels = eval_pred\n\n    # Apply sigmoid and threshold\n    predictions = torch.sigmoid(torch.tensor(predictions))\n    predictions = (predictions > 0.5).int().numpy()\n\n    # Global metrics\n    metrics = {\n        \"f1_macro\": f1_score(labels, predictions, average=\"macro\", zero_division=0),\n        \"f1_micro\": f1_score(labels, predictions, average=\"micro\", zero_division=0),\n        \"f1_weighted\": f1_score(\n            labels, predictions, average=\"weighted\", zero_division=0\n        ),\n        \"subset_accuracy\": accuracy_score(labels, predictions),\n        \"hamming_loss\": hamming_loss(labels, predictions),\n    }\n\n    # Per-category metrics\n    categories = [\"neurological\", \"cardiovascular\", \"hepatorenal\", \"oncological\"]\n    for i, cat in enumerate(categories):\n        cat_labels = labels[:, i]\n        cat_preds = predictions[:, i]\n\n        metrics[f\"f1_{cat}\"] = f1_score(cat_labels, cat_preds, zero_division=0)\n        metrics[f\"precision_{cat}\"] = precision_score(\n            cat_labels, cat_preds, zero_division=0\n        )\n        metrics[f\"recall_{cat}\"] = recall_score(cat_labels, cat_preds, zero_division=0)\n\n    return metrics"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZeQOuKqhSNie"
   },
   "outputs": [],
   "source": "class MedicalPapersDataset(Dataset):\n    \"\"\"Custom dataset ensuring correct data types for multi-label classification\"\"\"\n\n    def __init__(self, texts, labels, tokenizer, max_length=512):\n        self.texts = texts.reset_index(drop=True)\n        self.labels = labels.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts.iloc[idx])\n        labels = self.labels.iloc[idx]\n\n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_length,\n            return_tensors=\"pt\",\n        )\n\n        return {\n            \"input_ids\": encoding[\"input_ids\"].flatten(),\n            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n            \"labels\": torch.tensor(labels, dtype=torch.float32),\n        }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4BFY2VY6cKo"
   },
   "outputs": [],
   "source": "class ImprovedTrainer(Trainer):\n    \"\"\"Trainer con pérdida BCE ponderada para desbalance de clases\"\"\"\n\n    def __init__(self, pos_weights=None, **kwargs):\n        super().__init__(**kwargs)\n        self.pos_weights = (\n            pos_weights.cuda()\n            if pos_weights is not None and torch.cuda.is_available()\n            else pos_weights\n        )\n\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n\n        # BCE loss con pesos de posición para balancear clases\n        if self.pos_weights is not None:\n            loss = torch.nn.functional.binary_cross_entropy_with_logits(\n                outputs.logits, labels, pos_weight=self.pos_weights\n            )\n        else:\n            loss = torch.nn.functional.binary_cross_entropy_with_logits(\n                outputs.logits, labels\n            )\n\n        return (loss, outputs) if return_outputs else loss"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvy5xSpN6eC3"
   },
   "outputs": [],
   "source": "def calculate_class_weights(df):\n    \"\"\"Calcula pesos para balancear clases automáticamente\"\"\"\n    labels_array = np.array(df['labels'].tolist())\n    pos_counts = labels_array.sum(axis=0)\n    neg_counts = len(labels_array) - pos_counts\n\n    # Evitar división por cero y calcular pesos\n    pos_weights = neg_counts / np.maximum(pos_counts, 1)\n\n    # Normalizar pesos para evitar valores extremos\n    pos_weights = np.clip(pos_weights, 0.1, 10.0)\n\n    categories = ['neurological', 'cardiovascular', 'hepatorenal', 'oncological']\n    print(\"\\nPesos calculados para balancear clases:\")\n    for i, (cat, weight) in enumerate(zip(categories, pos_weights)):\n        freq = pos_counts[i] / len(df) * 100\n        print(f\"  {cat:15}: peso {weight:.2f} (frecuencia: {freq:.1f}%)\")\n\n    return torch.tensor(pos_weights, dtype=torch.float32)\n\n\ndef create_medical_text_variation(original_text):\n    \"\"\"\n    Data augmentation específica para textos médicos multi-label\n    Basada en el análisis de 81 ejemplos reales con 3-4 categorías\n    \"\"\"\n\n    # 1. SINÓNIMOS ESPECÍFICOS DE TUS DATOS REALES\n    medical_synonyms = {\n        # Términos de toxicidad (patrón clave en tus datos)\n        'toxicity': ['adverse effects', 'side effects', 'toxic effects', 'harmful effects'],\n        'nephrotoxicity': ['renal toxicity', 'kidney damage', 'renal adverse effects'],\n        'hepatotoxicity': ['liver toxicity', 'hepatic damage', 'liver adverse effects'],\n        'cardiotoxicity': ['cardiac toxicity', 'heart damage', 'cardiovascular toxicity'],\n\n        # Términos de pacientes (235 menciones en tus datos)\n        'patient': ['subject', 'individual', 'case', 'participant'],\n        'patients': ['subjects', 'individuals', 'cases', 'participants'],\n\n        # Términos de tratamiento (frecuentes en tus datos)\n        'treatment': ['therapy', 'intervention', 'management', 'therapeutic approach'],\n        'therapy': ['treatment', 'intervention', 'therapeutic regimen'],\n        'drug': ['medication', 'pharmaceutical agent', 'therapeutic agent'],\n        'chemotherapy': ['anticancer treatment', 'cytotoxic therapy', 'oncological treatment'],\n\n        # Términos de órganos (patrones multi-organ)\n        'cardiac': ['cardiovascular', 'heart-related', 'myocardial'],\n        'renal': ['kidney-related', 'nephrological'],\n        'hepatic': ['liver-related', 'hepatological'],\n        'neurological': ['neurologic', 'brain-related', 'cerebral'],\n\n        # Términos de resultado\n        'failure': ['dysfunction', 'impairment', 'insufficiency'],\n        'dysfunction': ['impairment', 'abnormal function', 'malfunction'],\n        'syndrome': ['condition', 'disorder', 'clinical syndrome'],\n        'disease': ['disorder', 'condition', 'pathology'],\n\n        # Términos de severidad\n        'severe': ['serious', 'significant', 'marked', 'pronounced'],\n        'acute': ['sudden onset', 'rapid', 'abrupt'],\n        'chronic': ['long-term', 'persistent', 'prolonged']\n    }\n\n    # 2. FRASES MÉDICAS ESPECÍFICAS (basadas en estructura de tus textos)\n    medical_transitions = [\n        'Clinical presentation revealed ',\n        'Laboratory findings showed ',\n        'The patient developed ',\n        'Treatment resulted in ',\n        'Complications included ',\n        'Adverse effects comprised ',\n        'Multiple organ involvement included ',\n        'Systemic toxicity manifested as ',\n        'Multi-organ dysfunction presented with '\n    ]\n\n    # 3. PATRONES DE CO-OCURRENCIA (de tu análisis)\n    co_occurrence_patterns = {\n        'cardio_renal': ['cardiac and renal complications', 'cardiovascular-renal syndrome', 'cardio-renal toxicity'],\n        'neuro_cardio': ['neurological and cardiac effects', 'cerebro-cardiovascular complications'],\n        'cancer_toxicity': ['chemotherapy-induced toxicity', 'anticancer drug adverse effects', 'oncological treatment complications'],\n        'multi_organ': ['multi-organ toxicity', 'systemic adverse effects', 'multiple organ dysfunction']\n    }\n\n    # 4. APLICAR TRANSFORMACIONES\n    words = original_text.split()\n    transformed_words = []\n\n    # Posibilidad de añadir frase médica específica (20% probabilidad)\n    if random.random() < 0.2:\n        transition = random.choice(medical_transitions)\n        # Asegurarse de que no duplique el inicio\n        if not any(t.lower().strip() in original_text.lower()[:100] for t in medical_transitions):\n            words = [transition.strip()] + words\n\n    for word in words:\n        clean_word = word.lower().strip('.,!?():;[]\"')\n\n        # Reemplazar con sinónimo médico específico (25% probabilidad)\n        if clean_word in medical_synonyms and random.random() < 0.25:\n            synonym = random.choice(medical_synonyms[clean_word])\n            # Mantener capitalización original\n            if word[0].isupper():\n                synonym = synonym.capitalize()\n            transformed_words.append(word.replace(clean_word, synonym))\n        else:\n            transformed_words.append(word)\n\n    # 5. AÑADIR PATRONES DE CO-OCURRENCIA CONTEXTUAL\n    final_text = ' '.join(transformed_words)\n\n    # Si el texto contiene múltiples menciones de órganos, añadir patrón de co-ocurrencia\n    if random.random() < 0.15:  # 15% probabilidad\n        organ_mentions = 0\n        if any(term in final_text.lower() for term in ['cardiac', 'heart', 'cardiovascular']):\n            organ_mentions += 1\n        if any(term in final_text.lower() for term in ['renal', 'kidney', 'nephro']):\n            organ_mentions += 1\n        if any(term in final_text.lower() for term in ['hepatic', 'liver']):\n            organ_mentions += 1\n        if any(term in final_text.lower() for term in ['neuro', 'brain', 'cerebral']):\n            organ_mentions += 1\n\n        if organ_mentions >= 2:\n            # Añadir frase que enfatice la naturaleza multi-órgano\n            multi_organ_phrase = random.choice(co_occurrence_patterns['multi_organ'])\n            # Insertar en una posición lógica del texto\n            sentences = final_text.split('. ')\n            if len(sentences) > 1:\n                insert_pos = len(sentences) // 2\n                sentences.insert(insert_pos, f\"This case demonstrates {multi_organ_phrase}\")\n                final_text = '. '.join(sentences)\n\n    return final_text"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NsgvcnOrLJIX"
   },
   "outputs": [],
   "source": "def augment_multilabel_with_real_patterns(df, target_samples=40):\n    \"\"\"\n    Data augmentation específica para tus patrones multi-label reales\n    \"\"\"\n    print(\"Aplicando data augmentation basada en patrones reales...\")\n\n    # Identificar muestras multi-label\n    df['num_labels'] = df['labels'].apply(lambda x: sum(x))\n    df['label_combo'] = df['labels'].apply(lambda x: '|'.join([str(i) for i, v in enumerate(x) if v == 1]))\n\n    # Análisis de distribución actual\n    multilabel_df = df[df['num_labels'] > 1].copy()\n    combo_counts = multilabel_df['label_combo'].value_counts()\n\n    print(f\"Estado actual:\")\n    print(f\"   - Muestras multi-label: {len(multilabel_df)}\")\n    print(f\"   - Combinaciones únicas: {len(combo_counts)}\")\n\n    # Generar muestras sintéticas para combinaciones raras\n    augmented_samples = []\n    categories = ['neurological', 'cardiovascular', 'hepatorenal', 'oncological']\n\n    for combo, current_count in combo_counts.items():\n        if current_count < target_samples:\n            needed = target_samples - current_count\n            combo_data = multilabel_df[multilabel_df['label_combo'] == combo]\n\n            # Información de la combinación\n            combo_indices = [int(x) for x in combo.split('|')]\n            combo_names = [categories[i] for i in combo_indices]\n\n            print(f\"   {' + '.join(combo_names)}: {current_count} -> {target_samples} (+{needed})\")\n\n            for _ in range(needed):\n                # Seleccionar muestra base aleatoria\n                base_sample = combo_data.sample(1).iloc[0]\n\n                # Crear variación específica\n                augmented_text = create_medical_text_variation(base_sample['text'])\n\n                # Variación adicional: combinar con otro texto similar (10% probabilidad)\n                if random.random() < 0.1 and len(combo_data) > 1:\n                    other_sample = combo_data.sample(1).iloc[0]\n                    # Tomar primera mitad del texto original y segunda mitad de otro\n                    mid_point = len(augmented_text) // 2\n                    other_mid = len(other_sample['text']) // 2\n                    augmented_text = augmented_text[:mid_point] + \" Furthermore, \" + other_sample['text'][other_mid:]\n\n                augmented_samples.append({\n                    'text': augmented_text,\n                    'labels': base_sample['labels']\n                })\n\n    # Crear DataFrame final\n    if augmented_samples:\n        print(f\"Generadas {len(augmented_samples)} muestras sintéticas\")\n        augmented_df = pd.DataFrame(augmented_samples)\n        final_df = pd.concat([df[['text', 'labels']], augmented_df], ignore_index=True)\n\n        # Verificar distribución final\n        final_df['num_labels'] = final_df['labels'].apply(lambda x: sum(x))\n        final_multilabel = final_df[final_df['num_labels'] > 1]\n        print(f\"Resultado: {len(final_multilabel)} muestras multi-label ({len(final_multilabel)/len(final_df)*100:.1f}%)\")\n\n        return final_df\n    else:\n        print(\"No se generaron muestras adicionales\")\n        return df[['text', 'labels']]"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Y8WZAsm2LeRJ"
   },
   "outputs": [],
   "source": [
    "def create_targeted_synthetic_samples(df, focus_combinations):\n",
    "    \"\"\"\n",
    "    Crear muestras sintéticas específicas para las combinaciones más difíciles\n",
    "    \"\"\"\n",
    "    import random\n",
    "\n",
    "    # Templates específicos basados en tus 81 ejemplos reales\n",
    "    templates = {\n",
    "        'cardio_renal_onco': [\n",
    "            \"{drug} treatment in {cancer_type} patients resulted in {cardiac_effect} and {renal_effect}. {outcome}\",\n",
    "            \"Case report of {cancer_type} patient developing {cardiac_effect} and {renal_effect} following {drug} therapy. {complications}\",\n",
    "            \"{drug}-induced {cardiac_effect} and {renal_effect} in oncological patients with {cancer_type}. {management}\"\n",
    "        ],\n",
    "        'neuro_cardio_renal': [\n",
    "            \"Patient with {neuro_condition} developed {cardiac_effect} and {renal_effect} during treatment. {outcome}\",\n",
    "            \"{drug} therapy caused {neuro_effect}, {cardiac_effect}, and {renal_effect} in this clinical case. {management}\",\n",
    "            \"Multi-organ toxicity including {neuro_effect}, {cardiac_effect}, and {renal_effect} following {intervention}. {outcome}\"\n",
    "        ],\n",
    "        'all_four': [\n",
    "            \"Complex case of {cancer_type} patient with {neuro_condition} developing {cardiac_effect}, {renal_effect}, and {hepatic_effect}. {comprehensive_management}\",\n",
    "            \"{drug} treatment resulted in multi-system toxicity: {neuro_effect}, {cardiac_effect}, {hepatic_effect}, and {renal_effect}. {outcome}\",\n",
    "            \"Rare presentation of {syndrome} with neurological, cardiovascular, hepatic, and renal involvement. {clinical_course}\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Variables para los templates (extraídas de tus datos reales)\n",
    "    variables = {\n",
    "        'drug': ['doxorubicin', 'cisplatin', 'tacrolimus', 'amiodarone', 'lithium', 'phenytoin'],\n",
    "        'cancer_type': ['leukemia', 'lymphoma', 'carcinoma', 'sarcoma', 'breast cancer', 'lung cancer'],\n",
    "        'cardiac_effect': ['cardiotoxicity', 'arrhythmias', 'heart failure', 'myocardial dysfunction'],\n",
    "        'renal_effect': ['nephrotoxicity', 'acute renal failure', 'renal dysfunction', 'kidney damage'],\n",
    "        'hepatic_effect': ['hepatotoxicity', 'liver dysfunction', 'hepatic failure', 'liver damage'],\n",
    "        'neuro_effect': ['neurotoxicity', 'encephalopathy', 'seizures', 'cognitive impairment'],\n",
    "        'neuro_condition': ['stroke', 'epilepsy', 'dementia', 'Parkinson disease'],\n",
    "        'outcome': ['Patient recovered with supportive care.', 'Long-term monitoring required.', 'Partial recovery achieved.'],\n",
    "        'complications': ['Multiple complications required intensive management.', 'Severe adverse effects were observed.'],\n",
    "        'management': ['Treatment was discontinued and supportive care initiated.', 'Dose reduction and monitoring implemented.'],\n",
    "        'comprehensive_management': ['Multidisciplinary approach required for optimal outcomes.', 'Complex case requiring specialized care.'],\n",
    "        'intervention': ['chemotherapy', 'immunosuppressive therapy', 'antiarrhythmic treatment'],\n",
    "        'syndrome': ['multi-organ failure syndrome', 'drug-induced multi-system toxicity', 'complex clinical syndrome'],\n",
    "        'clinical_course': ['Progressive deterioration observed.', 'Gradual improvement with treatment modifications.']\n",
    "    }\n",
    "\n",
    "    synthetic_samples = []\n",
    "\n",
    "    for combo_name, template_list in templates.items():\n",
    "        for _ in range(5):  # 5 muestras por template\n",
    "            template = random.choice(template_list)\n",
    "\n",
    "            # Rellenar template con variables aleatorias\n",
    "            filled_template = template\n",
    "            for var_name, var_options in variables.items():\n",
    "                if f'{{{var_name}}}' in filled_template:\n",
    "                    filled_template = filled_template.replace(f'{{{var_name}}}', random.choice(var_options))\n",
    "\n",
    "            # Determinar labels según el template\n",
    "            if combo_name == 'cardio_renal_onco':\n",
    "                labels = [0, 1, 1, 1]  # cardiovascular, hepatorenal, oncological\n",
    "            elif combo_name == 'neuro_cardio_renal':\n",
    "                labels = [1, 1, 1, 0]  # neurological, cardiovascular, hepatorenal\n",
    "            elif combo_name == 'all_four':\n",
    "                labels = [1, 1, 1, 1]  # all categories\n",
    "\n",
    "            synthetic_samples.append({\n",
    "                'text': filled_template,\n",
    "                'labels': labels\n",
    "            })\n",
    "\n",
    "    return synthetic_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLMA5-ijLUWx"
   },
   "outputs": [],
   "source": "def prepare_medical_dataset_enhanced(df, apply_augmentation=True):\n    \"\"\"Tu función prepare_medical_dataset pero con augmentation mejorada\"\"\"\n\n    # Tu código original\n    category_mapping = {\n        'neurological': 0,\n        'cardiovascular': 1,\n        'hepatorenal': 2,\n        'oncological': 3\n    }\n\n    def parse_medical_labels(group_str):\n        labels = [0, 0, 0, 0]\n        if pd.isna(group_str):\n            return labels\n        categories = str(group_str).split('|')\n        for cat in categories:\n            cat = cat.strip().lower()\n            if cat in category_mapping:\n                labels[category_mapping[cat]] = 1\n        return labels\n\n    # Crear texto combinado\n    df['text'] = df['title'].astype(str) + \" [SEP] \" + df['abstract'].astype(str)\n    df['labels'] = df['group'].apply(parse_medical_labels)\n\n    # NUEVA PARTE: Augmentation mejorada\n    if apply_augmentation:\n        print(\"Aplicando data augmentation específica para multi-label...\")\n        df = augment_multilabel_with_real_patterns(df, target_samples=35)\n\n        # Añadir muestras sintéticas dirigidas\n        print(\"Creando muestras sintéticas para combinaciones críticas...\")\n        synthetic_samples = create_targeted_synthetic_samples(df, ['all_four', 'cardio_renal_onco'])\n        if synthetic_samples:\n            synthetic_df = pd.DataFrame(synthetic_samples)\n            df = pd.concat([df, synthetic_df], ignore_index=True)\n            print(f\"Añadidas {len(synthetic_samples)} muestras sintéticas dirigidas\")\n\n    # Imprimir distribución final\n    categories = ['neurological', 'cardiovascular', 'hepatorenal', 'oncological']\n    print(\"\\nDistribución final de etiquetas:\")\n    for i, cat in enumerate(categories):\n        count = sum(1 for labels in df['labels'] if labels[i] == 1)\n        percentage = (count / len(df)) * 100\n        print(f\"  {cat:15}: {count:4d} samples ({percentage:5.1f}%)\")\n\n    # Análisis multi-label\n    df['num_labels'] = df['labels'].apply(lambda x: sum(x))\n    multilabel_count = sum(1 for num in df['num_labels'] if num > 1)\n    print(f\"\\nMuestras multi-label: {multilabel_count} ({multilabel_count/len(df)*100:.1f}%)\")\n\n    return df[['text', 'labels']].copy()"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6o1pUy51LFZi"
   },
   "outputs": [],
   "source": [
    "def get_optimized_training_args():\n",
    "    \"\"\"Configuración optimizada para multi-label desbalanceado\"\"\"\n",
    "    return TrainingArguments(\n",
    "        output_dir='./pubmedbert-medical-v6',\n",
    "\n",
    "        # Entrenamiento más largo y cuidadoso\n",
    "        num_train_epochs=4,  # Más epochs para aprender patrones complejos\n",
    "        per_device_train_batch_size=6,  # Batch más pequeño para mejor gradientes\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "\n",
    "        # Evaluación más frecuente\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "        logging_steps=25,\n",
    "\n",
    "        # Optimización\n",
    "        fp16=True,\n",
    "        max_grad_norm=1.0,\n",
    "        learning_rate=2e-5,  # Learning rate más bajo para estabilidad\n",
    "        warmup_ratio=0.1,  # Más warmup para convergencia suave\n",
    "        weight_decay=0.1,  # Mayor regularización\n",
    "        lr_scheduler_type=\"cosine_with_restarts\",\n",
    "\n",
    "        # Early stopping mejorado\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1_macro\",\n",
    "        greater_is_better=True,\n",
    "\n",
    "        # Regularización adicional\n",
    "        label_smoothing_factor=0.05,  # Suavizado suave\n",
    "\n",
    "        # Configuración técnica\n",
    "        seed=42,\n",
    "        report_to=\"wandb\",\n",
    "        dataloader_num_workers=0,\n",
    "        remove_unused_columns=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zgfqDhj76hNG"
   },
   "outputs": [],
   "source": "def train_optimized_medical_classifier(csv_path, sep=\";\", quotechar='\"'):\n    \"\"\"\n    Función principal optimizada para resolver el problema de multi-label desbalanceado\n    \"\"\"\n\n    print(\"=== ENTRENAMIENTO OPTIMIZADO PARA MULTI-LABEL ===\")\n    check_gpu()\n\n    # 1. CARGAR DATOS\n    print(f\"\\nCargando datos desde {csv_path}\")\n    df = pd.read_csv(csv_path, sep=sep, quotechar=quotechar, quoting=1)\n    print(f\"Cargados {len(df):,} samples\")\n\n    # 2. CARGAR MODELO\n    print(\"\\nCargando PubMedBERT model...\")\n    tokenizer, model = load_model_and_tokenizer()\n\n    # 3. PREPARAR DATOS CON MEJORAS\n    print(\"\\nPreparando dataset con optimizaciones...\")\n    df_prepared = prepare_medical_dataset_enhanced(df, apply_augmentation=True)\n\n    # 5. CALCULAR PESOS DE CLASE\n    print(\"\\nCalculando pesos para balancear clases...\")\n    class_weights = calculate_class_weights(df_prepared)\n\n    # 6. ANÁLISIS DE TEXTO\n    optimal_max_length = 512  # analyze_text_lengths(df_prepared, tokenizer)\n\n    # 7. DIVISIÓN ESTRATIFICADA\n    print(\"\\nDividiendo datos con estratificación...\")\n\n    df_prepared['label_string'] = df_prepared['labels'].apply(str)\n    train_df, val_df = train_test_split(\n        df_prepared,\n        test_size=0.2,\n        stratify=df_prepared['label_string'],\n        random_state=42\n    )\n    print(f\"   Train: {len(train_df):,} samples\")\n    print(f\"   Validation: {len(val_df):,} samples\")\n\n    # Verificar distribución en validation\n    val_multilabel = sum(1 for labels in val_df['labels'] if sum(labels) > 1)\n    print(f\"   Multi-label en validation: {val_multilabel} ({val_multilabel/len(val_df)*100:.1f}%)\")\n\n    # 8. CREAR DATASETS\n    print(\"\\nCreando datasets optimizados...\")\n    train_dataset = MedicalPapersDataset(\n        train_df['text'], train_df['labels'], tokenizer, optimal_max_length\n    )\n    val_dataset = MedicalPapersDataset(\n        val_df['text'], val_df['labels'], tokenizer, optimal_max_length\n    )\n\n    # Verificar formato de datos\n    sample = train_dataset[0]\n    assert sample['labels'].dtype == torch.float32, \"Labels deben ser float32\"\n    print(\"Formato de datos verificado\")\n\n    # 9. CONFIGURAR ENTRENAMIENTO\n    training_args = get_optimized_training_args()\n\n    # 10. CREAR TRAINER OPTIMIZADO\n    print(\"\\nConfigurando trainer optimizado...\")\n    trainer = ImprovedTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        tokenizer=tokenizer,\n        data_collator=default_data_collator,\n        compute_metrics=compute_multilabel_metrics,\n        pos_weights=class_weights\n    )\n\n    # 11. ENTRENAR\n    print(f\"\\nIniciando entrenamiento optimizado...\")\n    print(f\"   Configuración:\")\n    print(f\"      - Epochs: {training_args.num_train_epochs}\")\n    print(f\"      - Batch efectivo: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n    print(f\"      - Learning rate: {training_args.learning_rate}\")\n    print(f\"      - Weight decay: {training_args.weight_decay}\")\n    print(f\"      - Max length: {optimal_max_length}\")\n    print(f\"      - Samples totales: {len(train_df):,}\")\n\n    train_result = trainer.train()\n\n    # 12. EVALUACIÓN FINAL\n    print(\"\\nEvaluación final...\")\n    final_metrics = trainer.evaluate()\n\n    # 13. MOSTRAR RESULTADOS\n    print(f\"\\n¡ENTRENAMIENTO COMPLETADO!\")\n    print(f\"\\nMétricas principales:\")\n    key_metrics = {\n        'eval_f1_macro': 'F1 Macro',\n        'eval_f1_micro': 'F1 Micro',\n        'eval_f1_weighted': 'F1 Weighted',\n        'eval_subset_accuracy': 'Subset Accuracy',\n        'eval_hamming_loss': 'Hamming Loss'\n    }\n\n    for metric_key, metric_name in key_metrics.items():\n        if metric_key in final_metrics:\n            value = final_metrics[metric_key]\n            print(f\"   {metric_name:15}: {value:.4f}\")\n\n    print(f\"\\nF1 Score por categoría:\")\n    categories = ['neurological', 'cardiovascular', 'hepatorenal', 'oncological']\n    for cat in categories:\n        f1_key = f'eval_f1_{cat}'\n        if f1_key in final_metrics:\n            print(f\"   {cat:15}: {final_metrics[f1_key]:.4f}\")\n\n    # 14. GUARDAR MODELO\n    model_path = \"./pubmedbert-medical-v6\"\n    trainer.save_model(model_path)\n    tokenizer.save_pretrained(model_path)\n    print(f\"\\nModelo guardado en: {model_path}\")\n\n    return trainer, final_metrics"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QpxiDKicL3Ze",
    "outputId": "68f89315-9b8f-4df6-a448-ebf76435ef70"
   },
   "outputs": [],
   "source": "if __name__ == \"__main__\":\n\n    print(\"Iniciando pipeline completo de optimización...\")\n\n    # 1. Entrenar modelo optimizado\n    csv_file = \"/content/challenge_data-18-ago.csv\"\n    trainer, metrics = train_optimized_medical_classifier(csv_file)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yhj1DTnqLuGs"
   },
   "source": [
    "# TESTING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "-zD8LGkTTF6Z",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "51db2b29-a20f-4400-9f9e-cd2d0b2fb187"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "neurological: 0.372\n",
      "cardiovascular: 0.912\n",
      "hepatorenal: 0.847\n",
      "oncological: 0.986\n"
     ]
    }
   ],
   "source": [
    "def predict_medical_categories(text, model_path=\"./pubmedbert-medical-v6\", threshold=0.5):\n",
    "    categories = ['neurological', 'cardiovascular', 'hepatorenal', 'oncological']\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512 # Same as training\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.sigmoid(outputs.logits)[0]\n",
    "\n",
    "    results = []\n",
    "    for i, (category, prob) in enumerate(zip(categories, predictions)):\n",
    "        results.append({\n",
    "            'category': category,\n",
    "            'probability': prob.item(),\n",
    "            'predicted': prob.item() > threshold\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test it\n",
    "sample_text = \"P53 inhibition exacerbates late-stage anthracycline cardiotoxicity. AIMS: Doxorubicin (DOX) is an effective anti-cancer therapeutic, but is associated with both acute and late-stage cardiotoxicity. Children are particularly sensitive to DOX-induced heart failure. Here, the impact of p53 inhibition on acute vs. late-stage DOX cardiotoxicity was examined in a juvenile model. METHODS AND RESULTS: Two-week-old MHC-CB7 mice (which express dominant-interfering p53 in cardiomyocytes) and their non-transgenic (NON-TXG) littermates received weekly DOX injections for 5 weeks (25 mg/kg cumulative dose). One week after the last DOX treatment (acute stage), MHC-CB7 mice exhibited improved cardiac function and lower levels of cardiomyocyte apoptosis when compared with the NON-TXG mice. Surprisingly, by 13 weeks following the last DOX treatment (late stage), MHC-CB7 exhibited a progressive decrease in cardiac function and higher rates of cardiomyocyte apoptosis when compared with NON-TXG mice. p53 inhibition blocked transient DOX-induced STAT3 activation in MHC-CB7 mice, which was associated with enhanced induction of the DNA repair proteins Ku70 and Ku80. Mice with cardiomyocyte-restricted deletion of STAT3 exhibited worse cardiac function, higher levels of cardiomyocyte apoptosis, and a greater induction of Ku70 and Ku80 in response to DOX treatment during the acute stage when compared with control animals. CONCLUSION: These data support a model wherein a p53-dependent cardioprotective pathway, mediated via STAT3 activation, mitigates DOX-induced myocardial stress during drug delivery. Furthermore, these data suggest an explanation as to how p53 inhibition can result in cardioprotection during drug treatment and, paradoxically, enhanced cardiotoxicity long after the cessation of drug treatment.\"\n",
    "\n",
    "for pred in predictions:\n",
    "    if pred['predicted']:\n",
    "        print(f\"{pred['category']}: {pred['probability']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "def predict_medical_categories(text, model_path=\"./pubmedbert-medical-v6\", threshold=0.25):\n    categories = ['neurological', 'cardiovascular', 'hepatorenal', 'oncological']\n\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n\n    inputs = tokenizer(\n        text,\n        return_tensors='pt',\n        truncation=True,\n        padding=True,\n        max_length=512\n    )\n\n    model.eval()\n    with torch.no_grad():\n        outputs = model(**inputs)\n        predictions = torch.sigmoid(outputs.logits)[0]\n\n    results = []\n    for i, (category, prob) in enumerate(zip(categories, predictions)):\n        results.append({\n            'category': category,\n            'probability': prob.item(),\n            'predicted': prob.item() > threshold\n        })\n\n    return results\n\n# ============================================================================\n# EASY TESTING - Just change the text below and run!\n# ============================================================================\n\n# CAMBIA ESTE TEXTO POR EL QUE QUIERAS PROBAR:\nmy_test_text = \"P53 inhibition exacerbates late-stage anthracycline cardiotoxicity. AIMS: Doxorubicin (DOX) is an effective anti-cancer therapeutic, but is associated with both acute and late-stage cardiotoxicity. Children are particularly sensitive to DOX-induced heart failure. Here, the impact of p53 inhibition on acute vs. late-stage DOX cardiotoxicity was examined in a juvenile model. METHODS AND RESULTS: Two-week-old MHC-CB7 mice (which express dominant-interfering p53 in cardiomyocytes) and their non-transgenic (NON-TXG) littermates received weekly DOX injections for 5 weeks (25 mg/kg cumulative dose). One week after the last DOX treatment (acute stage), MHC-CB7 mice exhibited improved cardiac function and lower levels of cardiomyocyte apoptosis when compared with the NON-TXG mice. Surprisingly, by 13 weeks following the last DOX treatment (late stage), MHC-CB7 exhibited a progressive decrease in cardiac function and higher rates of cardiomyocyte apoptosis when compared with NON-TXG mice. p53 inhibition blocked transient DOX-induced STAT3 activation in MHC-CB7 mice, which was associated with enhanced induction of the DNA repair proteins Ku70 and Ku80. Mice with cardiomyocyte-restricted deletion of STAT3 exhibited worse cardiac function, higher levels of cardiomyocyte apoptosis, and a greater induction of Ku70 and Ku80 in response to DOX treatment during the acute stage when compared with control animals. CONCLUSION: These data support a model wherein a p53-dependent cardioprotective pathway, mediated via STAT3 activation, mitigates DOX-induced myocardial stress during drug delivery. Furthermore, these data suggest an explanation as to how p53 inhibition can result in cardioprotection during drug treatment and, paradoxically, enhanced cardiotoxicity long after the cessation of drug treatment.\"\n\n# Ejecutar predicción\npredictions = predict_medical_categories(my_test_text)\n\n# Mostrar resultados\nprint(f\"Analyzing: {my_test_text[:60]}...\")\nprint(f\"\\nResults:\")\npredicted_count = 0\nfor pred in predictions:\n    status = \"+\" if pred['predicted'] else \"-\"\n    print(f\"  {status} {pred['category']:15}: {pred['probability']:.3f}\")\n    if pred['predicted']:\n        predicted_count += 1\n\nprint(f\"\\nSummary: {predicted_count} categories detected\")\nif predicted_count > 1:\n    print(\"MULTI-LABEL DETECTED!\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "clx6re8l0mH0",
    "outputId": "731a44cf-610f-4a5f-d2d4-1b552e1b03a2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import os\nimport shutil\nfrom google.colab import files\n\n# Lista de archivos necesarios\nfiles_needed = [\n    'config.json',\n    'model.safetensors',\n    'tokenizer.json',\n    'tokenizer_config.json',\n    'vocab.txt',\n    'special_tokens_map.json'\n]\n\n# Ruta de tu modelo\nmodel_path = \"./pubmedbert-medical-v6\"\n\n# Crear carpeta temporal para archivos a descargar\ndownload_folder = \"./model_files_to_download\"\nos.makedirs(download_folder, exist_ok=True)\n\n# Copiar solo los archivos necesarios\nfor file_name in files_needed:\n    source = os.path.join(model_path, file_name)\n    dest = os.path.join(download_folder, file_name)\n\n    if os.path.exists(source):\n        shutil.copy2(source, dest)\n        print(f\"+ Copiado: {file_name}\")\n    else:\n        print(f\"- No encontrado: {file_name}\")\n\n# Crear ZIP con solo los archivos necesarios\nshutil.make_archive(\"my_medical_model\", 'zip', download_folder)\n\n# Descargar el ZIP\nfiles.download(\"my_medical_model.zip\")\n\nprint(\"\\nArchivo my_medical_model.zip descargado con éxito!\")\nprint(\"Contiene solo los archivos esenciales para el modelo.\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "Mz3bRxMxwnn4",
    "outputId": "187c1639-c16f-4376-eccc-4b0d436bf9d6"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "healthpro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}