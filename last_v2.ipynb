{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Qj6zZ9K9R_wk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    default_data_collator\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score, hamming_loss, precision_score, recall_score\n",
        "from torch.utils.data import Dataset\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "X0DbUjlWSJCR"
      },
      "outputs": [],
      "source": [
        "def check_gpu():\n",
        "    \"\"\"Check GPU availability\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    else:\n",
        "        print(\"WARNING: No GPU available\")\n",
        "\n",
        "def load_model_and_tokenizer():\n",
        "    \"\"\"Load PubMedBERT model and tokenizer\"\"\"\n",
        "    model_name = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=4,\n",
        "        problem_type=\"multi_label_classification\"\n",
        "    )\n",
        "\n",
        "    print(f\"Model loaded: {model.num_parameters():,} parameters\")\n",
        "    return tokenizer, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sN6qAaOsNtCv"
      },
      "outputs": [],
      "source": [
        "def analyze_text_lengths(df, tokenizer):\n",
        "    \"\"\"Analyze text lengths for optimal max_length\"\"\"\n",
        "    lengths = df['text'].apply(lambda x: len(tokenizer.encode(str(x))))\n",
        "\n",
        "    print(f\"\\nText length analysis:\")\n",
        "    print(f\"  Mean: {lengths.mean():.0f} tokens\")\n",
        "    print(f\"  95th percentile: {lengths.quantile(0.95):.0f} tokens\")\n",
        "    print(f\"  Max: {lengths.max():.0f} tokens\")\n",
        "\n",
        "    optimal_length = min(512, int(lengths.quantile(0.95)))\n",
        "    print(f\"  Recommended max_length: {optimal_length}\")\n",
        "\n",
        "    return optimal_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0Ab9xRmnOVZa"
      },
      "outputs": [],
      "source": [
        "def compute_multilabel_metrics(eval_pred):\n",
        "    \"\"\"Compute comprehensive multi-label metrics\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    # Apply sigmoid and threshold\n",
        "    predictions = torch.sigmoid(torch.tensor(predictions))\n",
        "    predictions = (predictions > 0.5).int().numpy()\n",
        "\n",
        "    # Global metrics\n",
        "    metrics = {\n",
        "        'f1_macro': f1_score(labels, predictions, average='macro', zero_division=0),\n",
        "        'f1_micro': f1_score(labels, predictions, average='micro', zero_division=0),\n",
        "        'f1_weighted': f1_score(labels, predictions, average='weighted', zero_division=0),\n",
        "        'subset_accuracy': accuracy_score(labels, predictions),\n",
        "        'hamming_loss': hamming_loss(labels, predictions)\n",
        "    }\n",
        "\n",
        "    # Per-category metrics\n",
        "    categories = ['neurological', 'cardiovascular', 'hepatorenal', 'oncological']\n",
        "    for i, cat in enumerate(categories):\n",
        "        cat_labels = labels[:, i]\n",
        "        cat_preds = predictions[:, i]\n",
        "\n",
        "        metrics[f'f1_{cat}'] = f1_score(cat_labels, cat_preds, zero_division=0)\n",
        "        metrics[f'precision_{cat}'] = precision_score(cat_labels, cat_preds, zero_division=0)\n",
        "        metrics[f'recall_{cat}'] = recall_score(cat_labels, cat_preds, zero_division=0)\n",
        "\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZeQOuKqhSNie"
      },
      "outputs": [],
      "source": [
        "class MedicalPapersDataset(Dataset):\n",
        "    \"\"\"Custom dataset ensuring correct data types for multi-label classification\"\"\"\n",
        "\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.texts = texts.reset_index(drop=True)\n",
        "        self.labels = labels.reset_index(drop=True)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts.iloc[idx])\n",
        "        labels = self.labels.iloc[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(labels, dtype=torch.float32)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "a4BFY2VY6cKo"
      },
      "outputs": [],
      "source": [
        "class ImprovedTrainer(Trainer):\n",
        "    \"\"\"Trainer con p√©rdida BCE ponderada para desbalance de clases\"\"\"\n",
        "\n",
        "    def __init__(self, pos_weights=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.pos_weights = pos_weights.cuda() if pos_weights is not None and torch.cuda.is_available() else pos_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # BCE loss con pesos de posici√≥n para balancear clases\n",
        "        if self.pos_weights is not None:\n",
        "            loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
        "                outputs.logits, labels, pos_weight=self.pos_weights\n",
        "            )\n",
        "        else:\n",
        "            loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
        "                outputs.logits, labels\n",
        "            )\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dvy5xSpN6eC3"
      },
      "outputs": [],
      "source": [
        "def calculate_class_weights(df):\n",
        "    \"\"\"Calcula pesos para balancear clases autom√°ticamente\"\"\"\n",
        "    labels_array = np.array(df['labels'].tolist())\n",
        "    pos_counts = labels_array.sum(axis=0)\n",
        "    neg_counts = len(labels_array) - pos_counts\n",
        "\n",
        "    # Evitar divisi√≥n por cero y calcular pesos\n",
        "    pos_weights = neg_counts / np.maximum(pos_counts, 1)\n",
        "\n",
        "    # Normalizar pesos para evitar valores extremos\n",
        "    pos_weights = np.clip(pos_weights, 0.1, 10.0)\n",
        "\n",
        "    categories = ['neurological', 'cardiovascular', 'hepatorenal', 'oncological']\n",
        "    print(\"\\nüìä Pesos calculados para balancear clases:\")\n",
        "    for i, (cat, weight) in enumerate(zip(categories, pos_weights)):\n",
        "        freq = pos_counts[i] / len(df) * 100\n",
        "        print(f\"  {cat:15}: peso {weight:.2f} (frecuencia: {freq:.1f}%)\")\n",
        "\n",
        "    return torch.tensor(pos_weights, dtype=torch.float32)\n",
        "\n",
        "\n",
        "def create_medical_text_variation(original_text):\n",
        "    \"\"\"\n",
        "    Data augmentation espec√≠fica para textos m√©dicos multi-label\n",
        "    Basada en el an√°lisis de 81 ejemplos reales con 3-4 categor√≠as\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # 1. SIN√ìNIMOS ESPEC√çFICOS DE TUS DATOS REALES\n",
        "    medical_synonyms = {\n",
        "        # T√©rminos de toxicidad (patr√≥n clave en tus datos)\n",
        "        'toxicity': ['adverse effects', 'side effects', 'toxic effects', 'harmful effects'],\n",
        "        'nephrotoxicity': ['renal toxicity', 'kidney damage', 'renal adverse effects'],\n",
        "        'hepatotoxicity': ['liver toxicity', 'hepatic damage', 'liver adverse effects'],\n",
        "        'cardiotoxicity': ['cardiac toxicity', 'heart damage', 'cardiovascular toxicity'],\n",
        "\n",
        "        # T√©rminos de pacientes (235 menciones en tus datos)\n",
        "        'patient': ['subject', 'individual', 'case', 'participant'],\n",
        "        'patients': ['subjects', 'individuals', 'cases', 'participants'],\n",
        "\n",
        "        # T√©rminos de tratamiento (frecuentes en tus datos)\n",
        "        'treatment': ['therapy', 'intervention', 'management', 'therapeutic approach'],\n",
        "        'therapy': ['treatment', 'intervention', 'therapeutic regimen'],\n",
        "        'drug': ['medication', 'pharmaceutical agent', 'therapeutic agent'],\n",
        "        'chemotherapy': ['anticancer treatment', 'cytotoxic therapy', 'oncological treatment'],\n",
        "\n",
        "        # T√©rminos de √≥rganos (patrones multi-organ)\n",
        "        'cardiac': ['cardiovascular', 'heart-related', 'myocardial'],\n",
        "        'renal': ['kidney-related', 'nephrological'],\n",
        "        'hepatic': ['liver-related', 'hepatological'],\n",
        "        'neurological': ['neurologic', 'brain-related', 'cerebral'],\n",
        "\n",
        "        # T√©rminos de resultado\n",
        "        'failure': ['dysfunction', 'impairment', 'insufficiency'],\n",
        "        'dysfunction': ['impairment', 'abnormal function', 'malfunction'],\n",
        "        'syndrome': ['condition', 'disorder', 'clinical syndrome'],\n",
        "        'disease': ['disorder', 'condition', 'pathology'],\n",
        "\n",
        "        # T√©rminos de severidad\n",
        "        'severe': ['serious', 'significant', 'marked', 'pronounced'],\n",
        "        'acute': ['sudden onset', 'rapid', 'abrupt'],\n",
        "        'chronic': ['long-term', 'persistent', 'prolonged']\n",
        "    }\n",
        "\n",
        "    # 2. FRASES M√âDICAS ESPEC√çFICAS (basadas en estructura de tus textos)\n",
        "    medical_transitions = [\n",
        "        'Clinical presentation revealed ',\n",
        "        'Laboratory findings showed ',\n",
        "        'The patient developed ',\n",
        "        'Treatment resulted in ',\n",
        "        'Complications included ',\n",
        "        'Adverse effects comprised ',\n",
        "        'Multiple organ involvement included ',\n",
        "        'Systemic toxicity manifested as ',\n",
        "        'Multi-organ dysfunction presented with '\n",
        "    ]\n",
        "\n",
        "    # 3. PATRONES DE CO-OCURRENCIA (de tu an√°lisis)\n",
        "    co_occurrence_patterns = {\n",
        "        'cardio_renal': ['cardiac and renal complications', 'cardiovascular-renal syndrome', 'cardio-renal toxicity'],\n",
        "        'neuro_cardio': ['neurological and cardiac effects', 'cerebro-cardiovascular complications'],\n",
        "        'cancer_toxicity': ['chemotherapy-induced toxicity', 'anticancer drug adverse effects', 'oncological treatment complications'],\n",
        "        'multi_organ': ['multi-organ toxicity', 'systemic adverse effects', 'multiple organ dysfunction']\n",
        "    }\n",
        "\n",
        "    # 4. APLICAR TRANSFORMACIONES\n",
        "    words = original_text.split()\n",
        "    transformed_words = []\n",
        "\n",
        "    # Posibilidad de a√±adir frase m√©dica espec√≠fica (20% probabilidad)\n",
        "    if random.random() < 0.2:\n",
        "        transition = random.choice(medical_transitions)\n",
        "        # Asegurarse de que no duplique el inicio\n",
        "        if not any(t.lower().strip() in original_text.lower()[:100] for t in medical_transitions):\n",
        "            words = [transition.strip()] + words\n",
        "\n",
        "    for word in words:\n",
        "        clean_word = word.lower().strip('.,!?():;[]\"')\n",
        "\n",
        "        # Reemplazar con sin√≥nimo m√©dico espec√≠fico (25% probabilidad)\n",
        "        if clean_word in medical_synonyms and random.random() < 0.25:\n",
        "            synonym = random.choice(medical_synonyms[clean_word])\n",
        "            # Mantener capitalizaci√≥n original\n",
        "            if word[0].isupper():\n",
        "                synonym = synonym.capitalize()\n",
        "            transformed_words.append(word.replace(clean_word, synonym))\n",
        "        else:\n",
        "            transformed_words.append(word)\n",
        "\n",
        "    # 5. A√ëADIR PATRONES DE CO-OCURRENCIA CONTEXTUAL\n",
        "    final_text = ' '.join(transformed_words)\n",
        "\n",
        "    # Si el texto contiene m√∫ltiples menciones de √≥rganos, a√±adir patr√≥n de co-ocurrencia\n",
        "    if random.random() < 0.15:  # 15% probabilidad\n",
        "        organ_mentions = 0\n",
        "        if any(term in final_text.lower() for term in ['cardiac', 'heart', 'cardiovascular']):\n",
        "            organ_mentions += 1\n",
        "        if any(term in final_text.lower() for term in ['renal', 'kidney', 'nephro']):\n",
        "            organ_mentions += 1\n",
        "        if any(term in final_text.lower() for term in ['hepatic', 'liver']):\n",
        "            organ_mentions += 1\n",
        "        if any(term in final_text.lower() for term in ['neuro', 'brain', 'cerebral']):\n",
        "            organ_mentions += 1\n",
        "\n",
        "        if organ_mentions >= 2:\n",
        "            # A√±adir frase que enfatice la naturaleza multi-√≥rgano\n",
        "            multi_organ_phrase = random.choice(co_occurrence_patterns['multi_organ'])\n",
        "            # Insertar en una posici√≥n l√≥gica del texto\n",
        "            sentences = final_text.split('. ')\n",
        "            if len(sentences) > 1:\n",
        "                insert_pos = len(sentences) // 2\n",
        "                sentences.insert(insert_pos, f\"This case demonstrates {multi_organ_phrase}\")\n",
        "                final_text = '. '.join(sentences)\n",
        "\n",
        "    return final_text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsgvcnOrLJIX"
      },
      "outputs": [],
      "source": [
        "def augment_multilabel_with_real_patterns(df, target_samples=40):\n",
        "    \"\"\"\n",
        "    Data augmentation espec√≠fica para tus patrones multi-label reales\n",
        "    \"\"\"\n",
        "    print(\"Aplicando data augmentation basada en patrones reales...\")\n",
        "\n",
        "    # Identificar muestras multi-label\n",
        "    df['num_labels'] = df['labels'].apply(lambda x: sum(x))\n",
        "    df['label_combo'] = df['labels'].apply(lambda x: '|'.join([str(i) for i, v in enumerate(x) if v == 1]))\n",
        "\n",
        "    # An√°lisis de distribuci√≥n actual\n",
        "    multilabel_df = df[df['num_labels'] > 1].copy()\n",
        "    combo_counts = multilabel_df['label_combo'].value_counts()\n",
        "\n",
        "    print(f\"Estado actual:\")\n",
        "    print(f\"   - Muestras multi-label: {len(multilabel_df)}\")\n",
        "    print(f\"   - Combinaciones √∫nicas: {len(combo_counts)}\")\n",
        "\n",
        "    # Generar muestras sint√©ticas para combinaciones raras\n",
        "    augmented_samples = []\n",
        "    categories = ['neurological', 'cardiovascular', 'hepatorenal', 'oncological']\n",
        "\n",
        "    for combo, current_count in combo_counts.items():\n",
        "        if current_count < target_samples:\n",
        "            needed = target_samples - current_count\n",
        "            combo_data = multilabel_df[multilabel_df['label_combo'] == combo]\n",
        "\n",
        "            # Informaci√≥n de la combinaci√≥n\n",
        "            combo_indices = [int(x) for x in combo.split('|')]\n",
        "            combo_names = [categories[i] for i in combo_indices]\n",
        "\n",
        "            print(f\"   {' + '.join(combo_names)}: {current_count} ‚Üí {target_samples} (+{needed})\")\n",
        "\n",
        "            for _ in range(needed):\n",
        "                # Seleccionar muestra base aleatoria\n",
        "                base_sample = combo_data.sample(1).iloc[0]\n",
        "\n",
        "                # Crear variaci√≥n espec√≠fica\n",
        "                augmented_text = create_medical_text_variation(base_sample['text'])\n",
        "\n",
        "                # Variaci√≥n adicional: combinar con otro texto similar (10% probabilidad)\n",
        "                if random.random() < 0.1 and len(combo_data) > 1:\n",
        "                    other_sample = combo_data.sample(1).iloc[0]\n",
        "                    # Tomar primera mitad del texto original y segunda mitad de otro\n",
        "                    mid_point = len(augmented_text) // 2\n",
        "                    other_mid = len(other_sample['text']) // 2\n",
        "                    augmented_text = augmented_text[:mid_point] + \" Furthermore, \" + other_sample['text'][other_mid:]\n",
        "\n",
        "                augmented_samples.append({\n",
        "                    'text': augmented_text,\n",
        "                    'labels': base_sample['labels']\n",
        "                })\n",
        "\n",
        "    # Crear DataFrame final\n",
        "    if augmented_samples:\n",
        "        print(f\" Generadas {len(augmented_samples)} muestras sint√©ticas\")\n",
        "        augmented_df = pd.DataFrame(augmented_samples)\n",
        "        final_df = pd.concat([df[['text', 'labels']], augmented_df], ignore_index=True)\n",
        "\n",
        "        # Verificar distribuci√≥n final\n",
        "        final_df['num_labels'] = final_df['labels'].apply(lambda x: sum(x))\n",
        "        final_multilabel = final_df[final_df['num_labels'] > 1]\n",
        "        print(f\" Resultado: {len(final_multilabel)} muestras multi-label ({len(final_multilabel)/len(final_df)*100:.1f}%)\")\n",
        "\n",
        "        return final_df\n",
        "    else:\n",
        "        print(\" No se generaron muestras adicionales\")\n",
        "        return df[['text', 'labels']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Y8WZAsm2LeRJ"
      },
      "outputs": [],
      "source": [
        "def create_targeted_synthetic_samples(df, focus_combinations):\n",
        "    \"\"\"\n",
        "    Crear muestras sint√©ticas espec√≠ficas para las combinaciones m√°s dif√≠ciles\n",
        "    \"\"\"\n",
        "    import random\n",
        "\n",
        "    # Templates espec√≠ficos basados en tus 81 ejemplos reales\n",
        "    templates = {\n",
        "        'cardio_renal_onco': [\n",
        "            \"{drug} treatment in {cancer_type} patients resulted in {cardiac_effect} and {renal_effect}. {outcome}\",\n",
        "            \"Case report of {cancer_type} patient developing {cardiac_effect} and {renal_effect} following {drug} therapy. {complications}\",\n",
        "            \"{drug}-induced {cardiac_effect} and {renal_effect} in oncological patients with {cancer_type}. {management}\"\n",
        "        ],\n",
        "        'neuro_cardio_renal': [\n",
        "            \"Patient with {neuro_condition} developed {cardiac_effect} and {renal_effect} during treatment. {outcome}\",\n",
        "            \"{drug} therapy caused {neuro_effect}, {cardiac_effect}, and {renal_effect} in this clinical case. {management}\",\n",
        "            \"Multi-organ toxicity including {neuro_effect}, {cardiac_effect}, and {renal_effect} following {intervention}. {outcome}\"\n",
        "        ],\n",
        "        'all_four': [\n",
        "            \"Complex case of {cancer_type} patient with {neuro_condition} developing {cardiac_effect}, {renal_effect}, and {hepatic_effect}. {comprehensive_management}\",\n",
        "            \"{drug} treatment resulted in multi-system toxicity: {neuro_effect}, {cardiac_effect}, {hepatic_effect}, and {renal_effect}. {outcome}\",\n",
        "            \"Rare presentation of {syndrome} with neurological, cardiovascular, hepatic, and renal involvement. {clinical_course}\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Variables para los templates (extra√≠das de tus datos reales)\n",
        "    variables = {\n",
        "        'drug': ['doxorubicin', 'cisplatin', 'tacrolimus', 'amiodarone', 'lithium', 'phenytoin'],\n",
        "        'cancer_type': ['leukemia', 'lymphoma', 'carcinoma', 'sarcoma', 'breast cancer', 'lung cancer'],\n",
        "        'cardiac_effect': ['cardiotoxicity', 'arrhythmias', 'heart failure', 'myocardial dysfunction'],\n",
        "        'renal_effect': ['nephrotoxicity', 'acute renal failure', 'renal dysfunction', 'kidney damage'],\n",
        "        'hepatic_effect': ['hepatotoxicity', 'liver dysfunction', 'hepatic failure', 'liver damage'],\n",
        "        'neuro_effect': ['neurotoxicity', 'encephalopathy', 'seizures', 'cognitive impairment'],\n",
        "        'neuro_condition': ['stroke', 'epilepsy', 'dementia', 'Parkinson disease'],\n",
        "        'outcome': ['Patient recovered with supportive care.', 'Long-term monitoring required.', 'Partial recovery achieved.'],\n",
        "        'complications': ['Multiple complications required intensive management.', 'Severe adverse effects were observed.'],\n",
        "        'management': ['Treatment was discontinued and supportive care initiated.', 'Dose reduction and monitoring implemented.'],\n",
        "        'comprehensive_management': ['Multidisciplinary approach required for optimal outcomes.', 'Complex case requiring specialized care.'],\n",
        "        'intervention': ['chemotherapy', 'immunosuppressive therapy', 'antiarrhythmic treatment'],\n",
        "        'syndrome': ['multi-organ failure syndrome', 'drug-induced multi-system toxicity', 'complex clinical syndrome'],\n",
        "        'clinical_course': ['Progressive deterioration observed.', 'Gradual improvement with treatment modifications.']\n",
        "    }\n",
        "\n",
        "    synthetic_samples = []\n",
        "\n",
        "    for combo_name, template_list in templates.items():\n",
        "        for _ in range(5):  # 5 muestras por template\n",
        "            template = random.choice(template_list)\n",
        "\n",
        "            # Rellenar template con variables aleatorias\n",
        "            filled_template = template\n",
        "            for var_name, var_options in variables.items():\n",
        "                if f'{{{var_name}}}' in filled_template:\n",
        "                    filled_template = filled_template.replace(f'{{{var_name}}}', random.choice(var_options))\n",
        "\n",
        "            # Determinar labels seg√∫n el template\n",
        "            if combo_name == 'cardio_renal_onco':\n",
        "                labels = [0, 1, 1, 1]  # cardiovascular, hepatorenal, oncological\n",
        "            elif combo_name == 'neuro_cardio_renal':\n",
        "                labels = [1, 1, 1, 0]  # neurological, cardiovascular, hepatorenal\n",
        "            elif combo_name == 'all_four':\n",
        "                labels = [1, 1, 1, 1]  # all categories\n",
        "\n",
        "            synthetic_samples.append({\n",
        "                'text': filled_template,\n",
        "                'labels': labels\n",
        "            })\n",
        "\n",
        "    return synthetic_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLMA5-ijLUWx"
      },
      "outputs": [],
      "source": [
        "def prepare_medical_dataset_enhanced(df, apply_augmentation=True):\n",
        "    \"\"\"Tu funci√≥n prepare_medical_dataset pero con augmentation mejorada\"\"\"\n",
        "\n",
        "    # Tu c√≥digo original\n",
        "    category_mapping = {\n",
        "        'neurological': 0,\n",
        "        'cardiovascular': 1,\n",
        "        'hepatorenal': 2,\n",
        "        'oncological': 3\n",
        "    }\n",
        "\n",
        "    def parse_medical_labels(group_str):\n",
        "        labels = [0, 0, 0, 0]\n",
        "        if pd.isna(group_str):\n",
        "            return labels\n",
        "        categories = str(group_str).split('|')\n",
        "        for cat in categories:\n",
        "            cat = cat.strip().lower()\n",
        "            if cat in category_mapping:\n",
        "                labels[category_mapping[cat]] = 1\n",
        "        return labels\n",
        "\n",
        "    # Crear texto combinado\n",
        "    df['text'] = df['title'].astype(str) + \" [SEP] \" + df['abstract'].astype(str)\n",
        "    df['labels'] = df['group'].apply(parse_medical_labels)\n",
        "\n",
        "    # NUEVA PARTE: Augmentation mejorada\n",
        "    if apply_augmentation:\n",
        "        print(\"üî¨ Aplicando data augmentation espec√≠fica para multi-label...\")\n",
        "        df = augment_multilabel_with_real_patterns(df, target_samples=35)\n",
        "\n",
        "        # A√±adir muestras sint√©ticas dirigidas\n",
        "        print(\" Creando muestras sint√©ticas para combinaciones cr√≠ticas...\")\n",
        "        synthetic_samples = create_targeted_synthetic_samples(df, ['all_four', 'cardio_renal_onco'])\n",
        "        if synthetic_samples:\n",
        "            synthetic_df = pd.DataFrame(synthetic_samples)\n",
        "            df = pd.concat([df, synthetic_df], ignore_index=True)\n",
        "            print(f\"‚ú® A√±adidas {len(synthetic_samples)} muestras sint√©ticas dirigidas\")\n",
        "\n",
        "    # Imprimir distribuci√≥n final\n",
        "    categories = ['neurological', 'cardiovascular', 'hepatorenal', 'oncological']\n",
        "    print(\"\\nDistribuci√≥n final de etiquetas:\")\n",
        "    for i, cat in enumerate(categories):\n",
        "        count = sum(1 for labels in df['labels'] if labels[i] == 1)\n",
        "        percentage = (count / len(df)) * 100\n",
        "        print(f\"  {cat:15}: {count:4d} samples ({percentage:5.1f}%)\")\n",
        "\n",
        "    # An√°lisis multi-label\n",
        "    df['num_labels'] = df['labels'].apply(lambda x: sum(x))\n",
        "    multilabel_count = sum(1 for num in df['num_labels'] if num > 1)\n",
        "    print(f\"\\nMuestras multi-label: {multilabel_count} ({multilabel_count/len(df)*100:.1f}%)\")\n",
        "\n",
        "    return df[['text', 'labels']].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6o1pUy51LFZi"
      },
      "outputs": [],
      "source": [
        "def get_optimized_training_args():\n",
        "    \"\"\"Configuraci√≥n optimizada para multi-label desbalanceado\"\"\"\n",
        "    return TrainingArguments(\n",
        "        output_dir='./pubmedbert-medical-v6',\n",
        "\n",
        "        # Entrenamiento m√°s largo y cuidadoso\n",
        "        num_train_epochs=4,  # M√°s epochs para aprender patrones complejos\n",
        "        per_device_train_batch_size=6,  # Batch m√°s peque√±o para mejor gradientes\n",
        "        per_device_eval_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "\n",
        "        # Evaluaci√≥n m√°s frecuente\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=50,\n",
        "        logging_steps=25,\n",
        "\n",
        "        # Optimizaci√≥n\n",
        "        fp16=True,\n",
        "        max_grad_norm=1.0,\n",
        "        learning_rate=2e-5,  # Learning rate m√°s bajo para estabilidad\n",
        "        warmup_ratio=0.1,  # M√°s warmup para convergencia suave\n",
        "        weight_decay=0.1,  # Mayor regularizaci√≥n\n",
        "        lr_scheduler_type=\"cosine_with_restarts\",\n",
        "\n",
        "        # Early stopping mejorado\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=50,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1_macro\",\n",
        "        greater_is_better=True,\n",
        "\n",
        "        # Regularizaci√≥n adicional\n",
        "        label_smoothing_factor=0.05,  # Suavizado suave\n",
        "\n",
        "        # Configuraci√≥n t√©cnica\n",
        "        seed=42,\n",
        "        report_to=\"wandb\",\n",
        "        dataloader_num_workers=0,\n",
        "        remove_unused_columns=False\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgfqDhj76hNG"
      },
      "outputs": [],
      "source": [
        "def train_optimized_medical_classifier(csv_path, sep=\";\", quotechar='\"'):\n",
        "    \"\"\"\n",
        "    Funci√≥n principal optimizada para resolver el problema de multi-label desbalanceado\n",
        "    \"\"\"\n",
        "\n",
        "    print(\" === ENTRENAMIENTO OPTIMIZADO PARA MULTI-LABEL ===\")\n",
        "    check_gpu()\n",
        "\n",
        "    # 1. CARGAR DATOS\n",
        "    print(f\"\\n Cargando datos desde {csv_path}\")\n",
        "    df = pd.read_csv(csv_path, sep=sep, quotechar=quotechar, quoting=1)\n",
        "    print(f\"‚úÖ Cargados {len(df):,} samples\")\n",
        "\n",
        "    # 2. CARGAR MODELO\n",
        "    print(\"\\n Cargando PubMedBERT model...\")\n",
        "    tokenizer, model = load_model_and_tokenizer()\n",
        "\n",
        "    # 3. PREPARAR DATOS CON MEJORAS\n",
        "    print(\"\\n Preparando dataset con optimizaciones...\")\n",
        "    df_prepared = prepare_medical_dataset_enhanced(df, apply_augmentation=True)\n",
        "\n",
        "    # 5. CALCULAR PESOS DE CLASE\n",
        "    print(\"\\n Calculando pesos para balancear clases...\")\n",
        "    class_weights = calculate_class_weights(df_prepared)\n",
        "\n",
        "    # 6. AN√ÅLISIS DE TEXTO\n",
        "    optimal_max_length = 512 #analyze_text_lengths(df_prepared, tokenizer)\n",
        "\n",
        "    # 7. DIVISI√ìN ESTRATIFICADA\n",
        "    print(\"\\n Dividiendo datos con estratificaci√≥n...\")\n",
        "\n",
        "    df_prepared['label_string'] = df_prepared['labels'].apply(str)\n",
        "    train_df, val_df = train_test_split(\n",
        "        df_prepared,\n",
        "        test_size=0.2,\n",
        "        stratify=df_prepared['label_string'],\n",
        "        random_state=42\n",
        "    )\n",
        "    print(f\"   Train: {len(train_df):,} samples\")\n",
        "    print(f\"   Validation: {len(val_df):,} samples\")\n",
        "\n",
        "    # Verificar distribuci√≥n en validation\n",
        "    val_multilabel = sum(1 for labels in val_df['labels'] if sum(labels) > 1)\n",
        "    print(f\"   Multi-label en validation: {val_multilabel} ({val_multilabel/len(val_df)*100:.1f}%)\")\n",
        "\n",
        "    # 8. CREAR DATASETS\n",
        "    print(\"\\n Creando datasets optimizados...\")\n",
        "    train_dataset = MedicalPapersDataset(\n",
        "        train_df['text'], train_df['labels'], tokenizer, optimal_max_length\n",
        "    )\n",
        "    val_dataset = MedicalPapersDataset(\n",
        "        val_df['text'], val_df['labels'], tokenizer, optimal_max_length\n",
        "    )\n",
        "\n",
        "    # Verificar formato de datos\n",
        "    sample = train_dataset[0]\n",
        "    assert sample['labels'].dtype == torch.float32, \"Labels deben ser float32\"\n",
        "    print(\" Formato de datos verificado\")\n",
        "\n",
        "    # 9. CONFIGURAR ENTRENAMIENTO\n",
        "    training_args = get_optimized_training_args()\n",
        "\n",
        "    # 10. CREAR TRAINER OPTIMIZADO\n",
        "    print(\"\\n Configurando trainer optimizado...\")\n",
        "    trainer = ImprovedTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=default_data_collator,\n",
        "        compute_metrics=compute_multilabel_metrics,\n",
        "        pos_weights=class_weights\n",
        "    )\n",
        "\n",
        "    # 11. ENTRENAR\n",
        "    print(f\"\\n Iniciando entrenamiento optimizado...\")\n",
        "    print(f\"    Configuraci√≥n:\")\n",
        "    print(f\"      - Epochs: {training_args.num_train_epochs}\")\n",
        "    print(f\"      - Batch efectivo: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "    print(f\"      - Learning rate: {training_args.learning_rate}\")\n",
        "    print(f\"      - Weight decay: {training_args.weight_decay}\")\n",
        "    print(f\"      - Max length: {optimal_max_length}\")\n",
        "    print(f\"      - Samples totales: {len(train_df):,}\")\n",
        "\n",
        "    train_result = trainer.train()\n",
        "\n",
        "    # 12. EVALUACI√ìN FINAL\n",
        "    print(\"\\n Evaluaci√≥n final...\")\n",
        "    final_metrics = trainer.evaluate()\n",
        "\n",
        "    # 13. MOSTRAR RESULTADOS\n",
        "    print(f\"\\n ¬°ENTRENAMIENTO COMPLETADO!\")\n",
        "    print(f\"\\n M√©tricas principales:\")\n",
        "    key_metrics = {\n",
        "        'eval_f1_macro': 'F1 Macro',\n",
        "        'eval_f1_micro': 'F1 Micro',\n",
        "        'eval_f1_weighted': 'F1 Weighted',\n",
        "        'eval_subset_accuracy': 'Subset Accuracy',\n",
        "        'eval_hamming_loss': 'Hamming Loss'\n",
        "    }\n",
        "\n",
        "    for metric_key, metric_name in key_metrics.items():\n",
        "        if metric_key in final_metrics:\n",
        "            value = final_metrics[metric_key]\n",
        "            print(f\"   {metric_name:15}: {value:.4f}\")\n",
        "\n",
        "    print(f\"\\n F1 Score por categor√≠a:\")\n",
        "    categories = ['neurological', 'cardiovascular', 'hepatorenal', 'oncological']\n",
        "    for cat in categories:\n",
        "        f1_key = f'eval_f1_{cat}'\n",
        "        if f1_key in final_metrics:\n",
        "            print(f\"   {cat:15}: {final_metrics[f1_key]:.4f}\")\n",
        "\n",
        "    # 14. GUARDAR MODELO\n",
        "    model_path = \"./pubmedbert-medical-v6\"\n",
        "    trainer.save_model(model_path)\n",
        "    tokenizer.save_pretrained(model_path)\n",
        "    print(f\"\\n Modelo guardado en: {model_path}\")\n",
        "\n",
        "    return trainer, final_metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QpxiDKicL3Ze",
        "outputId": "68f89315-9b8f-4df6-a448-ebf76435ef70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üé¨ Iniciando pipeline completo de optimizaci√≥n...\n",
            "üöÄ === ENTRENAMIENTO OPTIMIZADO PARA MULTI-LABEL ===\n",
            "GPU: Tesla T4\n",
            "VRAM: 14.7 GB\n",
            "\n",
            "üìÇ Cargando datos desde /content/challenge_data-18-ago.csv\n",
            "‚úÖ Cargados 3,565 samples\n",
            "\n",
            "üß† Cargando PubMedBERT model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded: 109,485,316 parameters\n",
            "\n",
            "‚öôÔ∏è Preparando dataset con optimizaciones...\n",
            "üî¨ Aplicando data augmentation espec√≠fica para multi-label...\n",
            "üöÄ Aplicando data augmentation basada en patrones reales...\n",
            "üìä Estado actual:\n",
            "   - Muestras multi-label: 1092\n",
            "   - Combinaciones √∫nicas: 11\n",
            "   üîß neurological + cardiovascular + hepatorenal: 28 ‚Üí 35 (+7)\n",
            "   üîß neurological + hepatorenal + oncological: 26 ‚Üí 35 (+9)\n",
            "   üîß neurological + cardiovascular + oncological: 13 ‚Üí 35 (+22)\n",
            "   üîß cardiovascular + hepatorenal + oncological: 7 ‚Üí 35 (+28)\n",
            "   üîß neurological + cardiovascular + hepatorenal + oncological: 7 ‚Üí 35 (+28)\n",
            "‚ú® Generadas 94 muestras sint√©ticas\n",
            "üìà Resultado: 1186 muestras multi-label (32.4%)\n",
            "üéØ Creando muestras sint√©ticas para combinaciones cr√≠ticas...\n",
            "‚ú® A√±adidas 15 muestras sint√©ticas dirigidas\n",
            "\n",
            "Distribuci√≥n final de etiquetas:\n",
            "  neurological   : 1861 samples ( 50.7%)\n",
            "  cardiovascular : 1368 samples ( 37.2%)\n",
            "  hepatorenal    : 1178 samples ( 32.1%)\n",
            "  oncological    :  698 samples ( 19.0%)\n",
            "\n",
            "Muestras multi-label: 1201 (32.7%)\n",
            "\n",
            "‚öñÔ∏è Calculando pesos para balancear clases...\n",
            "\n",
            "üìä Pesos calculados para balancear clases:\n",
            "  neurological   : peso 0.97 (frecuencia: 50.7%)\n",
            "  cardiovascular : peso 1.69 (frecuencia: 37.2%)\n",
            "  hepatorenal    : peso 2.12 (frecuencia: 32.1%)\n",
            "  oncological    : peso 4.26 (frecuencia: 19.0%)\n",
            "\n",
            "üìä Dividiendo datos con estratificaci√≥n...\n",
            "   Train: 2,939 samples\n",
            "   Validation: 735 samples\n",
            "   Multi-label en validation: 240 (32.7%)\n",
            "\n",
            "üîß Creando datasets optimizados...\n",
            "‚úÖ Formato de datos verificado\n",
            "\n",
            "üèãÔ∏è Configurando trainer optimizado...\n",
            "\n",
            "üéØ Iniciando entrenamiento optimizado...\n",
            "   üìã Configuraci√≥n:\n",
            "      - Epochs: 4\n",
            "      - Batch efectivo: 24\n",
            "      - Learning rate: 2e-05\n",
            "      - Weight decay: 0.1\n",
            "      - Max length: 512\n",
            "      - Samples totales: 2,939\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdzience\u001b[0m (\u001b[33mdzience-nousgraph\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250825_054800-a77cq269</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dzience-nousgraph/huggingface/runs/a77cq269' target=\"_blank\">genial-lion-20</a></strong> to <a href='https://wandb.ai/dzience-nousgraph/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/dzience-nousgraph/huggingface' target=\"_blank\">https://wandb.ai/dzience-nousgraph/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/dzience-nousgraph/huggingface/runs/a77cq269' target=\"_blank\">https://wandb.ai/dzience-nousgraph/huggingface/runs/a77cq269</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='492' max='492' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [492/492 08:20, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>F1 Micro</th>\n",
              "      <th>F1 Weighted</th>\n",
              "      <th>Subset Accuracy</th>\n",
              "      <th>Hamming Loss</th>\n",
              "      <th>F1 Neurological</th>\n",
              "      <th>Precision Neurological</th>\n",
              "      <th>Recall Neurological</th>\n",
              "      <th>F1 Cardiovascular</th>\n",
              "      <th>Precision Cardiovascular</th>\n",
              "      <th>Recall Cardiovascular</th>\n",
              "      <th>F1 Hepatorenal</th>\n",
              "      <th>Precision Hepatorenal</th>\n",
              "      <th>Recall Hepatorenal</th>\n",
              "      <th>F1 Oncological</th>\n",
              "      <th>Precision Oncological</th>\n",
              "      <th>Recall Oncological</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "      <th>Steps Per Second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.880500</td>\n",
              "      <td>0.854814</td>\n",
              "      <td>0.510864</td>\n",
              "      <td>0.546579</td>\n",
              "      <td>0.554108</td>\n",
              "      <td>0.170068</td>\n",
              "      <td>0.374150</td>\n",
              "      <td>0.697674</td>\n",
              "      <td>0.673317</td>\n",
              "      <td>0.723861</td>\n",
              "      <td>0.583867</td>\n",
              "      <td>0.449704</td>\n",
              "      <td>0.832117</td>\n",
              "      <td>0.412776</td>\n",
              "      <td>0.488372</td>\n",
              "      <td>0.357447</td>\n",
              "      <td>0.349138</td>\n",
              "      <td>0.249231</td>\n",
              "      <td>0.582734</td>\n",
              "      <td>6.883400</td>\n",
              "      <td>106.779000</td>\n",
              "      <td>26.731000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.568600</td>\n",
              "      <td>0.426395</td>\n",
              "      <td>0.844038</td>\n",
              "      <td>0.849875</td>\n",
              "      <td>0.850153</td>\n",
              "      <td>0.653061</td>\n",
              "      <td>0.102381</td>\n",
              "      <td>0.859504</td>\n",
              "      <td>0.883853</td>\n",
              "      <td>0.836461</td>\n",
              "      <td>0.837782</td>\n",
              "      <td>0.957746</td>\n",
              "      <td>0.744526</td>\n",
              "      <td>0.880165</td>\n",
              "      <td>0.855422</td>\n",
              "      <td>0.906383</td>\n",
              "      <td>0.798701</td>\n",
              "      <td>0.727811</td>\n",
              "      <td>0.884892</td>\n",
              "      <td>6.799300</td>\n",
              "      <td>108.100000</td>\n",
              "      <td>27.062000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.339100</td>\n",
              "      <td>0.293226</td>\n",
              "      <td>0.904844</td>\n",
              "      <td>0.901508</td>\n",
              "      <td>0.901366</td>\n",
              "      <td>0.772789</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.882022</td>\n",
              "      <td>0.926254</td>\n",
              "      <td>0.841823</td>\n",
              "      <td>0.908088</td>\n",
              "      <td>0.914815</td>\n",
              "      <td>0.901460</td>\n",
              "      <td>0.918919</td>\n",
              "      <td>0.976077</td>\n",
              "      <td>0.868085</td>\n",
              "      <td>0.910345</td>\n",
              "      <td>0.874172</td>\n",
              "      <td>0.949640</td>\n",
              "      <td>6.901300</td>\n",
              "      <td>106.502000</td>\n",
              "      <td>26.662000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.267500</td>\n",
              "      <td>0.238320</td>\n",
              "      <td>0.920477</td>\n",
              "      <td>0.917753</td>\n",
              "      <td>0.917431</td>\n",
              "      <td>0.802721</td>\n",
              "      <td>0.055782</td>\n",
              "      <td>0.885755</td>\n",
              "      <td>0.934524</td>\n",
              "      <td>0.841823</td>\n",
              "      <td>0.946396</td>\n",
              "      <td>0.958801</td>\n",
              "      <td>0.934307</td>\n",
              "      <td>0.936264</td>\n",
              "      <td>0.968182</td>\n",
              "      <td>0.906383</td>\n",
              "      <td>0.913495</td>\n",
              "      <td>0.880000</td>\n",
              "      <td>0.949640</td>\n",
              "      <td>6.822500</td>\n",
              "      <td>107.731000</td>\n",
              "      <td>26.969000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.226500</td>\n",
              "      <td>0.213687</td>\n",
              "      <td>0.931392</td>\n",
              "      <td>0.923853</td>\n",
              "      <td>0.922817</td>\n",
              "      <td>0.819048</td>\n",
              "      <td>0.051361</td>\n",
              "      <td>0.876437</td>\n",
              "      <td>0.944272</td>\n",
              "      <td>0.817694</td>\n",
              "      <td>0.955556</td>\n",
              "      <td>0.969925</td>\n",
              "      <td>0.941606</td>\n",
              "      <td>0.940171</td>\n",
              "      <td>0.944206</td>\n",
              "      <td>0.936170</td>\n",
              "      <td>0.953405</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.956835</td>\n",
              "      <td>6.826700</td>\n",
              "      <td>107.665000</td>\n",
              "      <td>26.953000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.210500</td>\n",
              "      <td>0.194170</td>\n",
              "      <td>0.936167</td>\n",
              "      <td>0.928393</td>\n",
              "      <td>0.927660</td>\n",
              "      <td>0.829932</td>\n",
              "      <td>0.048639</td>\n",
              "      <td>0.888260</td>\n",
              "      <td>0.940120</td>\n",
              "      <td>0.841823</td>\n",
              "      <td>0.952206</td>\n",
              "      <td>0.959259</td>\n",
              "      <td>0.945255</td>\n",
              "      <td>0.939914</td>\n",
              "      <td>0.948052</td>\n",
              "      <td>0.931915</td>\n",
              "      <td>0.964286</td>\n",
              "      <td>0.957447</td>\n",
              "      <td>0.971223</td>\n",
              "      <td>6.834400</td>\n",
              "      <td>107.544000</td>\n",
              "      <td>26.923000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.171600</td>\n",
              "      <td>0.182556</td>\n",
              "      <td>0.938091</td>\n",
              "      <td>0.930723</td>\n",
              "      <td>0.929957</td>\n",
              "      <td>0.834014</td>\n",
              "      <td>0.046939</td>\n",
              "      <td>0.892045</td>\n",
              "      <td>0.948640</td>\n",
              "      <td>0.841823</td>\n",
              "      <td>0.952555</td>\n",
              "      <td>0.952555</td>\n",
              "      <td>0.952555</td>\n",
              "      <td>0.943478</td>\n",
              "      <td>0.964444</td>\n",
              "      <td>0.923404</td>\n",
              "      <td>0.964286</td>\n",
              "      <td>0.957447</td>\n",
              "      <td>0.971223</td>\n",
              "      <td>6.736400</td>\n",
              "      <td>109.109000</td>\n",
              "      <td>27.314000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.136300</td>\n",
              "      <td>0.176267</td>\n",
              "      <td>0.943921</td>\n",
              "      <td>0.935936</td>\n",
              "      <td>0.935239</td>\n",
              "      <td>0.844898</td>\n",
              "      <td>0.043537</td>\n",
              "      <td>0.894217</td>\n",
              "      <td>0.943452</td>\n",
              "      <td>0.849866</td>\n",
              "      <td>0.959707</td>\n",
              "      <td>0.963235</td>\n",
              "      <td>0.956204</td>\n",
              "      <td>0.950538</td>\n",
              "      <td>0.960870</td>\n",
              "      <td>0.940426</td>\n",
              "      <td>0.971223</td>\n",
              "      <td>0.971223</td>\n",
              "      <td>0.971223</td>\n",
              "      <td>6.755100</td>\n",
              "      <td>108.806000</td>\n",
              "      <td>27.239000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.189200</td>\n",
              "      <td>0.174229</td>\n",
              "      <td>0.943706</td>\n",
              "      <td>0.936532</td>\n",
              "      <td>0.935892</td>\n",
              "      <td>0.846259</td>\n",
              "      <td>0.043197</td>\n",
              "      <td>0.898592</td>\n",
              "      <td>0.946588</td>\n",
              "      <td>0.855228</td>\n",
              "      <td>0.957952</td>\n",
              "      <td>0.959707</td>\n",
              "      <td>0.956204</td>\n",
              "      <td>0.950538</td>\n",
              "      <td>0.960870</td>\n",
              "      <td>0.940426</td>\n",
              "      <td>0.967742</td>\n",
              "      <td>0.964286</td>\n",
              "      <td>0.971223</td>\n",
              "      <td>6.728600</td>\n",
              "      <td>109.234000</td>\n",
              "      <td>27.346000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Evaluaci√≥n final...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='184' max='184' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [184/184 00:06]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üéâ ¬°ENTRENAMIENTO COMPLETADO!\n",
            "\n",
            "üìà M√©tricas principales:\n",
            "   F1 Macro       : 0.9439\n",
            "   F1 Micro       : 0.9359\n",
            "   F1 Weighted    : 0.9352\n",
            "   Subset Accuracy: 0.8449\n",
            "   Hamming Loss   : 0.0435\n",
            "\n",
            "üè∑Ô∏è F1 Score por categor√≠a:\n",
            "   neurological   : 0.8942\n",
            "   cardiovascular : 0.9597\n",
            "   hepatorenal    : 0.9505\n",
            "   oncological    : 0.9712\n",
            "\n",
            "üíæ Modelo guardado en: ./pubmedbert-medical-v6\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    print(\" Iniciando pipeline completo de optimizaci√≥n...\")\n",
        "\n",
        "    # 1. Entrenar modelo optimizado\n",
        "    csv_file = \"/content/challenge_data-18-ago.csv\"\n",
        "    trainer, metrics = train_optimized_medical_classifier(csv_file)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yhj1DTnqLuGs"
      },
      "source": [
        "# TESTING\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zD8LGkTTF6Z",
        "outputId": "51db2b29-a20f-4400-9f9e-cd2d0b2fb187"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "neurological: 0.372\n",
            "cardiovascular: 0.912\n",
            "hepatorenal: 0.847\n",
            "oncological: 0.986\n"
          ]
        }
      ],
      "source": [
        "def predict_medical_categories(text, model_path=\"./pubmedbert-medical-v6\", threshold=0.5):\n",
        "    categories = ['neurological', 'cardiovascular', 'hepatorenal', 'oncological']\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors='pt',\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=512 # Same as training\n",
        "    )\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.sigmoid(outputs.logits)[0]\n",
        "\n",
        "    results = []\n",
        "    for i, (category, prob) in enumerate(zip(categories, predictions)):\n",
        "        results.append({\n",
        "            'category': category,\n",
        "            'probability': prob.item(),\n",
        "            'predicted': prob.item() > threshold\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# Test it\n",
        "sample_text = \"P53 inhibition exacerbates late-stage anthracycline cardiotoxicity. AIMS: Doxorubicin (DOX) is an effective anti-cancer therapeutic, but is associated with both acute and late-stage cardiotoxicity. Children are particularly sensitive to DOX-induced heart failure. Here, the impact of p53 inhibition on acute vs. late-stage DOX cardiotoxicity was examined in a juvenile model. METHODS AND RESULTS: Two-week-old MHC-CB7 mice (which express dominant-interfering p53 in cardiomyocytes) and their non-transgenic (NON-TXG) littermates received weekly DOX injections for 5 weeks (25 mg/kg cumulative dose). One week after the last DOX treatment (acute stage), MHC-CB7 mice exhibited improved cardiac function and lower levels of cardiomyocyte apoptosis when compared with the NON-TXG mice. Surprisingly, by 13 weeks following the last DOX treatment (late stage), MHC-CB7 exhibited a progressive decrease in cardiac function and higher rates of cardiomyocyte apoptosis when compared with NON-TXG mice. p53 inhibition blocked transient DOX-induced STAT3 activation in MHC-CB7 mice, which was associated with enhanced induction of the DNA repair proteins Ku70 and Ku80. Mice with cardiomyocyte-restricted deletion of STAT3 exhibited worse cardiac function, higher levels of cardiomyocyte apoptosis, and a greater induction of Ku70 and Ku80 in response to DOX treatment during the acute stage when compared with control animals. CONCLUSION: These data support a model wherein a p53-dependent cardioprotective pathway, mediated via STAT3 activation, mitigates DOX-induced myocardial stress during drug delivery. Furthermore, these data suggest an explanation as to how p53 inhibition can result in cardioprotection during drug treatment and, paradoxically, enhanced cardiotoxicity long after the cessation of drug treatment.\"\n",
        "\n",
        "for pred in predictions:\n",
        "    if pred['predicted']:\n",
        "        print(f\"{pred['category']}: {pred['probability']:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clx6re8l0mH0",
        "outputId": "63e3df3b-bd22-4932-f1a3-07ee6aade7e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyzing: \tAdrenoleukodystrophy: survey of 303 cases: biochemistry, di...\n",
            "\n",
            "Results:\n",
            "  ‚úì neurological   : 0.868\n",
            "  ‚úó cardiovascular : 0.021\n",
            "  ‚úì hepatorenal    : 0.931\n",
            "  ‚úó oncological    : 0.036\n",
            "\n",
            "Summary: 2 categories detected\n",
            "MULTI-LABEL DETECTED!\n"
          ]
        }
      ],
      "source": [
        "def predict_medical_categories(text, model_path=\"./my_medical_model\", threshold=0.25):\n",
        "    categories = ['neurological', 'cardiovascular', 'hepatorenal', 'oncological']\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors='pt',\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.sigmoid(outputs.logits)[0]\n",
        "\n",
        "    results = []\n",
        "    for i, (category, prob) in enumerate(zip(categories, predictions)):\n",
        "        results.append({\n",
        "            'category': category,\n",
        "            'probability': prob.item(),\n",
        "            'predicted': prob.item() > threshold\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# ============================================================================\n",
        "# EASY TESTING - Just change the text below and run!\n",
        "# ============================================================================\n",
        "\n",
        "# CAMBIA ESTE TEXTO POR EL QUE QUIERAS PROBAR:\n",
        "my_test_text = \"\tAdrenoleukodystrophy: survey of 303 cases: biochemistry, diagnosis, and therapy. Adrenoleukodystrophy ( ALD ) is a genetically determined disorder associated with progressive central demyelination and adrenal cortical insufficiency . All affected persons show increased levels of saturated unbranched very-long-chain fatty acids , particularly hexacosanoate ( C26 0 ) , because of impaired capacity to degrade these acids . This degradation normally takes place in a subcellular organelle called the peroxisome , and ALD , together with Zellwegers cerebrohepatorenal syndrome , is now considered to belong to the newly formed category of peroxisomal disorders . Biochemical assays permit prenatal diagnosis , as well as identification of most heterozygotes . We have identified 303 patients with ALD in 217 kindreds . These patients show a wide phenotypic variation . Sixty percent of patients had childhood ALD and 17 % adrenomyeloneuropathy , both of which are X-linked , with the gene mapped to Xq28 . Neonatal ALD , a distinct entity with autosomal recessive inheritance and points of resemblance to Zellwegers syndrome , accounted for 7 % of the cases . Although excess C26 0 in the brain of patients with ALD is partially of dietary origin , dietary C26 0 restriction did not produce clear benefit . Bone marrow transplant lowered the plasma C26 0 level but failed to arrest neurological progression .\"\n",
        "\n",
        "# Ejecutar predicci√≥n\n",
        "predictions = predict_medical_categories(my_test_text)\n",
        "\n",
        "# Mostrar resultados\n",
        "print(f\"Analyzing: {my_test_text[:60]}...\")\n",
        "print(f\"\\nResults:\")\n",
        "predicted_count = 0\n",
        "for pred in predictions:\n",
        "    status = \"‚úì\" if pred['predicted'] else \"‚úó\"\n",
        "    print(f\"  {status} {pred['category']:15}: {pred['probability']:.3f}\")\n",
        "    if pred['predicted']:\n",
        "        predicted_count += 1\n",
        "\n",
        "print(f\"\\nSummary: {predicted_count} categories detected\")\n",
        "if predicted_count > 1:\n",
        "    print(\"MULTI-LABEL DETECTED!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "Mz3bRxMxwnn4",
        "outputId": "187c1639-c16f-4376-eccc-4b0d436bf9d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Copiado: config.json\n",
            "‚úì Copiado: model.safetensors\n",
            "‚úì Copiado: tokenizer.json\n",
            "‚úì Copiado: tokenizer_config.json\n",
            "‚úì Copiado: vocab.txt\n",
            "‚úì Copiado: special_tokens_map.json\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_231502f9-388f-4c98-a80a-a7b20c643d0c\", \"my_medical_model.zip\", 406821911)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Archivo my_medical_model.zip descargado con √©xito!\n",
            "Contiene solo los archivos esenciales para el modelo.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Lista de archivos necesarios\n",
        "files_needed = [\n",
        "    'config.json',\n",
        "    'model.safetensors',\n",
        "    'tokenizer.json',\n",
        "    'tokenizer_config.json',\n",
        "    'vocab.txt',\n",
        "    'special_tokens_map.json'\n",
        "]\n",
        "\n",
        "# Ruta de tu modelo\n",
        "model_path = \"./pubmedbert-medical-v6\"\n",
        "\n",
        "# Crear carpeta temporal para archivos a descargar\n",
        "download_folder = \"./model_files_to_download\"\n",
        "os.makedirs(download_folder, exist_ok=True)\n",
        "\n",
        "# Copiar solo los archivos necesarios\n",
        "for file_name in files_needed:\n",
        "    source = os.path.join(model_path, file_name)\n",
        "    dest = os.path.join(download_folder, file_name)\n",
        "\n",
        "    if os.path.exists(source):\n",
        "        shutil.copy2(source, dest)\n",
        "        print(f\"‚úì Copiado: {file_name}\")\n",
        "    else:\n",
        "        print(f\"‚úó No encontrado: {file_name}\")\n",
        "\n",
        "# Crear ZIP con solo los archivos necesarios\n",
        "shutil.make_archive(\"my_medical_model\", 'zip', download_folder)\n",
        "\n",
        "# Descargar el ZIP\n",
        "files.download(\"my_medical_model.zip\")\n",
        "\n",
        "print(\"\\nArchivo my_medical_model.zip descargado con √©xito!\")\n",
        "print(\"Contiene solo los archivos esenciales para el modelo.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nousvenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
